{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tokenizers import ByteLevelBPETokenizer, trainers, pre_tokenizers, decoders\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'ĠĊ']\n",
      "257\n"
     ]
    }
   ],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Initialize Constants, Text Memory, and Tokenizer\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "device = 'mps' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# How long before you stop and think?\n",
    "focus_len = 21\n",
    "\n",
    "# How many positional embeddings?\n",
    "n_embd = 21\n",
    "\n",
    "# Constants for multi-head attention\n",
    "n_head = 3\n",
    "n_layer = 3\n",
    "\n",
    "# Define dropout\n",
    "dropout = 0.2\n",
    "\n",
    "# Initialize empty memory vector\n",
    "with open(\"v01_txt/text_memory.txt\", \"w\", encoding=\"utf-8\") as text_memory:\n",
    "    for _ in range(500):\n",
    "        text_memory.write(\" \\n\")\n",
    "\n",
    "# Initialize tokenizer trained on nothing\n",
    "init_tokenizer = ByteLevelBPETokenizer(vocab=None, merges=None)\n",
    "init_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "init_tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "init_tokenizer.train(\"v01_txt/text_memory.txt\", vocab_size=1000,)\n",
    "\n",
    "init_save_path = \"/Users/bennyrose/Desktop/Papers n Projects/Little Guy/v0/v01_tokenizer\"\n",
    "init_tokenizer.save(init_save_path)\n",
    "\n",
    "tokenizer = init_tokenizer\n",
    "\n",
    "# Get vocab and vocab length\n",
    "with open(\"v01_tokenizer\", 'r') as f:\n",
    "    json_content = f.read()\n",
    "\n",
    "parsed_json = json.loads(json_content)\n",
    "vocab = list(parsed_json[\"model\"][\"vocab\"].keys())\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab)\n",
    "print(vocab_size)\n",
    "\n",
    "# Initialize \"old_top_tokens\" for very first response\n",
    "old_top_tokens = [137, 137, 137]\n",
    "\n",
    "# How much are other q-values discounted if feedback was positive?\n",
    "q_discount_pos = 0.75\n",
    "\n",
    "# How much are other q-values discounted if feedback was neutral?\n",
    "q_discount_neut = 0.95\n",
    "\n",
    "# How much is chosen q-value discounted if feedback was negative?\n",
    "q_discount_neg = 0.5\n",
    "\n",
    "# Initialize \"old_top_tokens\" and \"old_action\" for very first response\n",
    "old_top_tokenids = [137, 137, 137]\n",
    "old_action = None\n",
    "\n",
    "# How many tokens to generate at a time?\n",
    "max_new_tokens = 40\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Initialize learnable embedding tables\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class EmbeddingTables(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(n_embd, n_embd)\n",
    "\n",
    "        # Initialize and save token embedding table\n",
    "        torch.nn.init.normal_(self.token_embedding_table.weight, mean=0.0, std=0.02)\n",
    "        # token_embedding_weight = self.token_embedding_table.weight.detach().numpy()\n",
    "        \n",
    "        with open(\"v01_pkl/token_embed.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.token_embedding_table, f)\n",
    "            # print(\"Saved token embedding table to token_embed.pkl\")\n",
    "\n",
    "        # Initialize and save position embedding table\n",
    "        torch.nn.init.normal_(self.position_embedding_table.weight, mean=0.0, std=0.02)\n",
    "        # position_embedding_weight = self.position_embedding_table.weight.detach().numpy()\n",
    "\n",
    "        with open(\"v01_pkl/position_embed.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.position_embedding_table, f)\n",
    "            # print(\"Saved position embedding table to position_embed.pkl\")\n",
    "\n",
    "    def update_embeds(self, vocab, new_vocab):\n",
    "        # Update token embedding table\n",
    "        with open('v01_pkl/token_embed.pkl', 'rb') as f:\n",
    "            self.token_embedding_table = pickle.load(f)\n",
    "\n",
    "        old_token_table = self.token_embedding_table.weight.detach().numpy()\n",
    "        old_token_embedding_table = torch.tensor(old_token_table)\n",
    "\n",
    "        new_token_embedding_table = old_token_embedding_table.clone()\n",
    "\n",
    "        for token in new_vocab:\n",
    "            if token not in vocab:\n",
    "                random_embedding = torch.empty(1, old_token_embedding_table.size(1)).normal_(mean=0.0, std=0.02)\n",
    "                new_token_embedding_table = torch.cat([new_token_embedding_table, random_embedding], dim=0)\n",
    "\n",
    "        nums, dims = new_token_embedding_table.size()\n",
    "        self.new_token_embedding_table = nn.Embedding(nums, dims)\n",
    "\n",
    "        with open('v01_pkl/token_embed.pkl', 'wb') as f:\n",
    "            pickle.dump(self.new_token_embedding_table, f)\n",
    "\n",
    "        # Update position embedding table\n",
    "        with open('v01_pkl/position_embed.pkl', 'rb') as f:\n",
    "            self.position_embedding_table = pickle.load(f)\n",
    "\n",
    "        old_position_table = self.position_embedding_table.weight.detach().numpy()\n",
    "        old_position_embedding_table = torch.tensor(old_position_table)\n",
    "\n",
    "        new_position_embedding_table = old_position_embedding_table.clone()\n",
    "\n",
    "        for token in new_vocab:\n",
    "            if token not in vocab:\n",
    "                random_embedding = torch.empty(1, old_position_embedding_table.size(1)).normal_(mean=0.0, std=0.02)\n",
    "                new_position_embedding_table = torch.cat([new_position_embedding_table, random_embedding], dim=0)\n",
    "        \n",
    "        nums, dims = new_position_embedding_table.size()\n",
    "        self.new_position_embedding_table = nn.Embedding(nums, dims)\n",
    "\n",
    "        with open('v01_pkl/position_embed.pkl', 'wb') as f:\n",
    "            pickle.dump(self.new_position_embedding_table, f)\n",
    "\n",
    "\n",
    "    # Function to get token and positional embeddings\n",
    "    def forward(self, index):\n",
    "        with open('v01_pkl/token_embed.pkl', 'rb') as f:\n",
    "            self.token_embedding_table = pickle.load(f)\n",
    "\n",
    "        with open('v01_pkl/position_embed.pkl', 'rb') as f:\n",
    "            self.position_embedding_table = pickle.load(f)\n",
    "\n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(len(index), device = device))\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        return x\n",
    "\n",
    "embeds = EmbeddingTables(vocab_size, n_embd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Initialize Feed Forward architecture\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Initialize Single-Head Attention for Learning\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class SingleHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(focus_len, focus_len)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "\n",
    "        x = self.ln1(x + out)\n",
    "        y = self.ffwd(x)\n",
    "        out = self.ln2(x + y)\n",
    "\n",
    "        return out\n",
    "    \n",
    "single_head = SingleHead(n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Initialize Q-Learning Architecture\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class QLearn():\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actions = [[1, 1, 1], [1, 1, 2],\n",
    "                        [1, 2, 1], [1, 2, 2],\n",
    "                        [2, 1, 1], [2, 1, 2],\n",
    "                        [2, 2, 1], [2, 2, 2], None]\n",
    "        \n",
    "        self.q_vals = torch.ones(vocab_size, vocab_size, vocab_size, len(self.actions), dtype=torch.float)\n",
    "        self.q_vals[:, :, :, :8] = 0.1\n",
    "        self.q_vals[:, :, :, 8] = 0.2\n",
    "\n",
    "        # with open(\"v0_pkl/q_vals.pkl\", \"wb\") as f:\n",
    "        #     pickle.dump(self.q_vals, f)\n",
    "\n",
    "    def update_q_matrix(self, vocab, new_vocab):\n",
    "        # Open saved q_val table\n",
    "        # with open('v0_pkl/q_vals.pkl', 'rb') as f:\n",
    "        #     self.q_vals = pickle.load(f)\n",
    "\n",
    "        # Create new table from updated vocab\n",
    "        new_q_vals = torch.ones(len(new_vocab), len(new_vocab), len(new_vocab), len(self.actions), dtype=torch.float)\n",
    "        new_q_vals[:, :, :, :8] = 0.1\n",
    "        new_q_vals[:, :, :, 8] = 0.2\n",
    "\n",
    "        # Copy values from the existing lookup table to the new one\n",
    "        for i, token_i in enumerate(vocab):\n",
    "            if token_i in new_vocab:\n",
    "                for j, token_j in enumerate(vocab):\n",
    "                    if token_j in new_vocab:\n",
    "                        for k, token_k in enumerate(vocab):\n",
    "                            if token_k in new_vocab:\n",
    "                                new_i = new_vocab.index(token_i)\n",
    "                                new_j = new_vocab.index(token_j)\n",
    "                                new_k = new_vocab.index(token_k)\n",
    "                                new_q_vals[new_i, new_j, new_k] = self.q_vals[i, j, k]\n",
    "\n",
    "        self.q_vals = new_q_vals\n",
    "\n",
    "        # Save updated table as q_vals\n",
    "        # with open('v0_pkl/q_vals.pkl', 'wb') as f:\n",
    "        #     pickle.dump(self.new_q_vals, f)\n",
    "\n",
    "    def update_q_values(self, old_top_tokenids, old_action, vocab):\n",
    "\n",
    "        # Update q-values based on feedback\n",
    "        with open('v01_txt/current_feedback.txt', 'r') as f:\n",
    "            feedback = f.read()\n",
    "\n",
    "        # with open('v0_pkl/q_vals.pkl', 'rb') as f:\n",
    "        #     self.q_vals = pickle.load(f)\n",
    "\n",
    "        action_index = self.actions.index(old_action)\n",
    "        \n",
    "        if feedback == \"1\":    # Positive feedback\n",
    "            for i, token_i in enumerate(vocab):\n",
    "                for j, token_j in enumerate(vocab):\n",
    "                    for k, token_k in enumerate(vocab):\n",
    "                        if ((i in old_top_tokenids) or (j in old_top_tokenids) or (k in old_top_tokenids)):\n",
    "                            sum = 0\n",
    "                            for _ in range(9):\n",
    "                                if _ != action_index:\n",
    "                                    self.q_vals[i, j, k, _] *= q_discount_pos\n",
    "                                    sum += self.q_vals[i, j, k, _]\n",
    "\n",
    "                            # sum = torch.round(sum, decimals=6)\n",
    "                            self.q_vals[i, j, k, action_index] = 1 - sum\n",
    "\n",
    "\n",
    "        elif feedback == \"2\":  # Neutral feedback\n",
    "            for i, token_i in enumerate(vocab):\n",
    "                for j, token_j in enumerate(vocab):\n",
    "                    for k, token_k in enumerate(vocab):\n",
    "                        if ((i in old_top_tokenids) or (j in old_top_tokenids) or (k in old_top_tokenids)):\n",
    "                            sum = 0\n",
    "                            for _ in range(9):\n",
    "                                if _ != action_index:\n",
    "                                    self.q_vals[i, j, k, _] *= q_discount_neut\n",
    "                                    sum += self.q_vals[i, j, k, _]\n",
    "\n",
    "                            # sum = torch.round(sum, decimals=6)\n",
    "                            self.q_vals[i, j, k, action_index] = 1 - sum\n",
    "            \n",
    "        else:                  # Negative feedback\n",
    "            for i, token_i in enumerate(vocab):\n",
    "                for j, token_j in enumerate(vocab):\n",
    "                    for k, token_k in enumerate(vocab):\n",
    "                        if ((i in old_top_tokenids) or (j in old_top_tokenids) or (k in old_top_tokenids)):\n",
    "                            new_val = self.q_vals[i, j, k, action_index] * q_discount_neg\n",
    "                            diff = self.q_vals[i, j, k, action_index] - new_val\n",
    "                            to_add = diff / 8          # Distribute difference among remaining 8 paths\n",
    "\n",
    "                            self.q_vals[i, j, k, action_index] = new_val\n",
    "                            for _ in range(9):\n",
    "                                if _ != action_index:\n",
    "                                    self.q_vals[i, j, k, _] += to_add\n",
    "\n",
    "\n",
    "            \n",
    "    def get_path(self, new_top_tokens):\n",
    "        # Load the appropriate q_vals\n",
    "        # with open('v0_pkl/q_vals.pkl', 'rb') as f:\n",
    "        #     self.q_vals = pickle.load(f)\n",
    "\n",
    "        # Get the probability distribution located at top_token indices\n",
    "        action_probs = self.q_vals[new_top_tokens[0], new_top_tokens[1], new_top_tokens[2]]\n",
    "        # print(action_probs)\n",
    "\n",
    "        # Select an action index from this distribution\n",
    "        # action_index = np.random.choice(len(action_probs), p=action_probs)\n",
    "        action_index = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "        # Get action from this index\n",
    "        action = self.actions[action_index]\n",
    "\n",
    "        return action\n",
    "    \n",
    "q_learn = QLearn(vocab_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Step 1 (L)\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "def L1():\n",
    "    # Start new \"current input\" record\n",
    "    with open(\"v01_txt/current_input.txt\", \"w\", encoding=\"utf-8\"):\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        # Retrieve Feedback and Input\n",
    "        emoji_choice = input(\"1. 🤩  2. 🙂  3. 😓\\n Enter Feedback, or Press 'return' to Exit: \")\n",
    "        \n",
    "        if emoji_choice == \"\":\n",
    "            break\n",
    "        if emoji_choice not in (\"1\", \"2\", \"3\"):\n",
    "            emoji_choice = \"2\"\n",
    "\n",
    "        # Map the user's choice to an emoji and emotion\n",
    "        emoji_mapping = {\n",
    "            '1': '🤩',\n",
    "            '2': '🙂',\n",
    "            '3': '😓'\n",
    "        }\n",
    "\n",
    "        emoji = emoji_mapping.get(emoji_choice)\n",
    "\n",
    "        emotion_mapping = {\n",
    "            '1': 'Positive',\n",
    "            '2': 'Neutral',\n",
    "            '3': 'Negative'\n",
    "        }\n",
    "\n",
    "        emotion = emotion_mapping.get(emoji_choice)\n",
    "        \n",
    "        # Open text window\n",
    "        user_input = input(f\"Say Something {emotion} {emoji}: \")\n",
    "\n",
    "        if user_input.endswith(\".txt\"):\n",
    "            print(\"Received a .txt file!\")\n",
    "            break\n",
    "\n",
    "        print(\"Received: \", emoji, user_input)\n",
    "\n",
    "        with open(\"v01_txt/current_feedback.txt\", \"w\"):\n",
    "            pass\n",
    "\n",
    "        with open(\"v01_txt/current_feedback.txt\", \"a\") as current_feedback:\n",
    "            current_feedback.write(emoji_choice)\n",
    "\n",
    "        with open(\"v01_txt/current_input.txt\", \"a\", encoding=\"utf-8\") as current_input:\n",
    "            current_input.write(user_input + \"\\n\")\n",
    "        line_nums = sum(1 for _ in open(\"v01_txt/current_input.txt\", \"r\", encoding=\"utf-8\"))\n",
    "        if line_nums > 5:\n",
    "            print(\"Let Little Guy Think!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Step 4 (L)\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def L4():\n",
    "\n",
    "    # Given current input, add to buffer\n",
    "    with open(\"v01_txt/current_input.txt\", \"r\", encoding=\"utf-8\") as current_input:\n",
    "\n",
    "        with open(\"v01_txt/buffer.txt\", \"a\", encoding=\"utf-8\") as buffer_file:\n",
    "            buffer_file.writelines(current_input) \n",
    "\n",
    "    # Given current input, add to daily buffer\n",
    "    with open(\"v01_txt/current_input.txt\", \"r\", encoding=\"utf-8\") as current_input:\n",
    "\n",
    "        with open(\"v01_txt/daily_buffer.txt\", \"a\", encoding=\"utf-8\") as daily_buffer:\n",
    "            daily_buffer.writelines(current_input)\n",
    "\n",
    "    \n",
    "    with open(\"v01_txt/buffer.txt\", \"r\", encoding=\"utf-8\") as buffer_file:\n",
    "        line_count = sum(1 for line in buffer_file)\n",
    "        # print(line_count) \n",
    "\n",
    "    # If buffer is full, add to text memory, empty buffer, and retokenize based on recent memory\n",
    "    if line_count >= 10:\n",
    "\n",
    "        push_text_replace(\"text_memory.txt\", \"buffer.txt\")\n",
    "\n",
    "        with open(\"v01_txt/buffer.txt\", \"w\", encoding=\"utf-8\") as buffer_file:\n",
    "            pass\n",
    "    \n",
    "        # Initialize tokenizer and train on recent text memory\n",
    "        new_tokenizer = ByteLevelBPETokenizer(vocab=None, merges=None)\n",
    "        new_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "        new_tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "        new_tokenizer.train(\"v01_txt/text_memory.txt\", vocab_size=1000,)\n",
    "\n",
    "        initial_save_path = \"/Users/bennyrose/Desktop/Papers n Projects/Little Guy/v0/new_tokenizer\"\n",
    "        new_tokenizer.save(initial_save_path)\n",
    "        \n",
    "        return new_tokenizer\n",
    "    \n",
    "\n",
    "##\n",
    "### Create function for altering .txt files\n",
    "##\n",
    "    \n",
    "def push_text_replace(memory, input):\n",
    "    # Read the lines from input1.txt\n",
    "    with open(f\"v01_txt/{memory}\", \"r\", encoding=\"utf-8\") as memory_file_read:\n",
    "        memory_lines = memory_file_read.readlines()\n",
    "        memory_file_read.close()\n",
    "\n",
    "\n",
    "    # Read the lines from text_memory.txt\n",
    "    with open(f\"v01_txt/{input}\", \"r\", encoding=\"utf-8\") as input_file_read:\n",
    "        input_lines = input_file_read.readlines()\n",
    "        input_file_read.close()\n",
    "\n",
    "    # Combine the lines from input1.txt and text_memory.txt\n",
    "    new_lines = memory_lines[len(input_lines):] + input_lines\n",
    "\n",
    "    # Write the combined lines back to text_memory.txt\n",
    "    with open(f\"v01_txt/{memory}\", \"w\", encoding=\"utf-8\") as memory_file_write:\n",
    "        memory_file_write.writelines(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Step 5 (L)\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def L5(tokenizer):\n",
    "    with open(\"v01_tokenizer\", \"r\") as file:\n",
    "        json_content = file.read()\n",
    "\n",
    "    parsed_json = json.loads(json_content)\n",
    "    new_vocab = list(parsed_json[\"model\"][\"vocab\"].keys())\n",
    "    new_vocab_len = len(parsed_json[\"model\"][\"vocab\"])\n",
    "\n",
    "    return new_vocab, new_vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Step 6 (L)\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def L6(tokenizer, focus_len):\n",
    "\n",
    "    with open(\"v01_txt/current_input.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "    # Encode text\n",
    "    tokens = tokenizer.encode(text_data).ids\n",
    "    data = torch.tensor(tokens)\n",
    "\n",
    "    if len(data) > focus_len:     # Truncate long entries\n",
    "        index = data[-focus_len:]\n",
    "    elif len(data) < focus_len:   # Pad short entries\n",
    "        empty_add = torch.tensor([256] * (focus_len - len(data)))\n",
    "        index = torch.cat([empty_add, data])\n",
    "    else: index = data\n",
    "\n",
    "    return index, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Step 7 (L)\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def GetTopTokens(attended, index, vocab):\n",
    "    avgs = torch.mean(torch.abs(attended), dim=1)\n",
    "    top_indices = torch.topk(torch.abs(avgs), k=3, dim=0)[1]\n",
    "\n",
    "    # Create a new tensor with 1s at the top 3 indices and 0s elsewhere\n",
    "    binary_tensor = torch.zeros_like(avgs)\n",
    "    binary_tensor.scatter_(0, top_indices, 1)\n",
    "    \n",
    "    top_tokens = binary_tensor.clone().detach().bool()\n",
    "\n",
    "    decoded_tokens = [tokenizer.decode([token_id]) for token_id in index.tolist()]\n",
    "\n",
    "    selected_tokens = [token for token, is_top in zip(decoded_tokens, top_tokens) if is_top]\n",
    "    selected_token_ids = [token_id for token_id, is_top in zip(index.tolist(), top_tokens) if is_top]\n",
    "\n",
    "    return selected_tokens, selected_token_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Define transformer blocks\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "\n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Define multi-head attention\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Define feed forward sequence\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Define individual attention head\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(focus_len, focus_len)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Initialize Dual-GPT Architecture\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "class DGPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        ##\n",
    "        ### Create block sequence 1 with distinct blocks\n",
    "        ##\n",
    "        self.block_11 = Block(n_embd, n_head=n_head)\n",
    "        self.block_12 = Block(n_embd, n_head=n_head)\n",
    "        self.block_13 = Block(n_embd, n_head=n_head)\n",
    "\n",
    "        self.blocks_1 = nn.Sequential(self.block_11, self.block_12, self.block_13)\n",
    "\n",
    "        self.seq1_blocks = [self.block_11, self.block_12, self.block_13]\n",
    "\n",
    "        ##\n",
    "        ### Create block sequence 2 with distinct blocks\n",
    "        ##\n",
    "        self.block_21 = Block(n_embd, n_head=n_head)\n",
    "        self.block_22 = Block(n_embd, n_head=n_head)\n",
    "        self.block_23 = Block(n_embd, n_head=n_head)\n",
    "\n",
    "        self.blocks_2 = nn.Sequential(self.block_21, self.block_22, self.block_23)\n",
    "\n",
    "        self.seq2_blocks = [self.block_21, self.block_22, self.block_23]\n",
    "\n",
    "        ##\n",
    "        ### Create layer norm and linear layer\n",
    "        ##\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self.__init__weights)\n",
    "\n",
    "    def __init__weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    ##\n",
    "    ### When vocab changes, add a new linear layer of correct size\n",
    "    ##\n",
    "    def update_linlayer(self, old_vocab_size, new_vocab_size):\n",
    "        new_lm_head = nn.Linear(old_vocab_size, new_vocab_size)\n",
    "        torch.nn.init.normal_(new_lm_head.weight, mean=0.0, std=0.02)\n",
    "        if new_lm_head.bias is not None:\n",
    "            torch.nn.init.zeros_(new_lm_head.bias)\n",
    "        \n",
    "        if isinstance(self.lm_head, nn.Linear):\n",
    "            self.lm_head = nn.Sequential(self.lm_head, new_lm_head)\n",
    "        else: self.lm_head = nn.Sequential(*self.lm_heah, new_lm_head)\n",
    "\n",
    "    ##\n",
    "    ### Train both major pathways\n",
    "    ##\n",
    "    def train_forward(self, embeds, index, targets=None):\n",
    "\n",
    "        x = embeds.forward(index)\n",
    "        \n",
    "        x1 = self.blocks_1(x)\n",
    "        x2 = self.blocks_2(x)\n",
    "        x1 = self.ln_f(x1)\n",
    "        x2 = self.ln_f(x2)\n",
    "        logits1 = self.lm_head(x1)\n",
    "        logits2 = self.lm_head(x2)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B1, T1, C1 = logits1.shape\n",
    "            logits1 = logits1.view(B1*T1, C1)\n",
    "            targets = targets.view(B1*T1)\n",
    "            loss1 = F.cross_entropy(logits1, targets)\n",
    "\n",
    "            B2, T2, C2 = logits2.shape\n",
    "            logits2 = logits2.view(B2*T2, C2)\n",
    "            targets = targets.view(B2*T2)\n",
    "            loss2 = F.cross_entropy(logits2, targets)\n",
    "\n",
    "        return logits1, loss1, logits2, loss2\n",
    "    \n",
    "    ##\n",
    "    ### Forward step for generation\n",
    "    ##\n",
    "    def generate_forward(self, embeds, index, targets=None):\n",
    "\n",
    "        x = embeds.forward(index)\n",
    "\n",
    "        x = self.curr_blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    ##\n",
    "    ### Select transformer sequence and generate\n",
    "    ##\n",
    "    def generate(self, embeds, index, path, max_new_tokens):\n",
    "        if path == None:\n",
    "            return None\n",
    "        else:\n",
    "            block_path = [self.seq2_blocks[i] if p == 2 else self.seq1_blocks[i] for i, p in enumerate(path)]\n",
    "            self.curr_blocks = nn.Sequential(*block_path)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.generate_forward(embeds, index, None)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "            index = index[:, 1:]\n",
    "        return index\n",
    "    \n",
    "dual_gpt = DGPT(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received:  🤩 I hope I fixed it!\n",
      "Little Guy's got nothin' to say!\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "### Get new input text\n",
    "## \n",
    "\n",
    "L1()\n",
    "\n",
    "##\n",
    "### Update Q-Values\n",
    "##\n",
    "q_learn.update_q_values(old_top_tokenids, old_action, vocab)\n",
    "\n",
    "##\n",
    "### Save input text and retokenize if needed\n",
    "##\n",
    "\n",
    "new_tokenizer = L4()\n",
    "\n",
    "##\n",
    "### Save new tokenizer that may have been generated in previous step\n",
    "### Additionally get new vocab and new vocab length\n",
    "##\n",
    "\n",
    "if new_tokenizer is not None:\n",
    "    tokenizer = new_tokenizer\n",
    "    tokenizer.save(\"v01_tokenizer\")\n",
    "\n",
    "    # Get new vocab\n",
    "    new_vocab, new_vocab_len = L5(tokenizer)\n",
    "\n",
    "    # Update embedding tables\n",
    "    embeds.update_embeds(vocab, new_vocab)\n",
    "\n",
    "    # Updating Q-Table\n",
    "    q_learn.update_q_matrix(vocab, new_vocab)\n",
    "\n",
    "    vocab = new_vocab\n",
    "\n",
    "##\n",
    "### Encode and get training text (index)\n",
    "##\n",
    "\n",
    "index, data = L6(tokenizer, focus_len)\n",
    "\n",
    "##\n",
    "### Get embedding of index\n",
    "##\n",
    "\n",
    "x = embeds.forward(index)\n",
    "\n",
    "##\n",
    "### Perform single-head attention on the embedding\n",
    "##\n",
    "\n",
    "attended = single_head.forward(x)\n",
    "\n",
    "##\n",
    "### Extract the top 3 priority tokens\n",
    "##\n",
    "\n",
    "new_top_tokens, new_top_tokenids = GetTopTokens(attended, index, vocab)\n",
    "\n",
    "##\n",
    "### Get transformer sequence from Q-values\n",
    "##\n",
    "\n",
    "path = q_learn.get_path(new_top_tokenids)\n",
    "\n",
    "##\n",
    "### Reset \"old\" variables from this step\n",
    "##\n",
    "\n",
    "old_action = path\n",
    "old_top_tokenids = new_top_tokenids\n",
    "\n",
    "if path is not None:\n",
    "    generated_chars = tokenizer.decode(dual_gpt.generate(embeds, index.unsqueeze(0), path, max_new_tokens)[0].tolist())\n",
    "    print(f\"Little Guy says: {generated_chars}\")\n",
    "else: print(\"Little Guy's got nothin' to say!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2]\n",
      "[' \\n', ' \\n', ' \\n']\n"
     ]
    }
   ],
   "source": [
    "print(path)\n",
    "print(new_top_tokens)\n",
    "# print(old_action)\n",
    "# print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "#  Get training batches from daily buffer\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Place in initialization block\n",
    "block_size = 32\n",
    "batch_size = 64\n",
    "\n",
    "### Place in training loop function\n",
    "with open(\"v01_txt/daily_buffer.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "    ### Encode daily buffer text\n",
    "    tokens = tokenizer.encode(text).ids\n",
    "    data = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "### For optimizer, dual_gpt.parameters() won't capture transformer sequences seperately\n",
    "### figure out how to optimize different sequences independently\n",
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "littleguy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
