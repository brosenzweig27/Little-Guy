{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tokenizers import ByteLevelBPETokenizer, trainers, pre_tokenizers, decoders, Tokenizer\n",
    "\n",
    "from arch import LGGPT, SingleHead, GetTopTokens\n",
    "from alpha import AlphaZeroTrainer\n",
    "from embed import EmbeddingTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = ByteLevelBPETokenizer(vocab=None, merges=None)\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "# tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# # Get vocab and vocab length\n",
    "# with open(\"new_tokenizer.json\", 'r') as f:\n",
    "#     json_content = f.read()\n",
    "\n",
    "# parsed_json = json.loads(json_content)\n",
    "# vocab = list(parsed_json[\"model\"][\"vocab\"].keys())\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"new_tokenizer.json\")\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "\n",
    "n_embd = 21\n",
    "n_head = 3\n",
    "dropout = 0.1\n",
    "focus_len = 256\n",
    "head_size = n_embd // n_head\n",
    "\n",
    "gpt = LGGPT(vocab_size, n_embd, n_head, dropout, focus_len)\n",
    "embeds = EmbeddingTables(vocab_size=vocab_size, n_embd=n_embd, focus_len=focus_len)\n",
    "single_head = SingleHead(n_embd, head_size, dropout, focus_len)\n",
    "trainer = AlphaZeroTrainer(model=gpt, embeds=embeds, tokenizer=tokenizer, single_head=single_head, vocab=vocab, lr=0.001)\n",
    "single_head = SingleHead(n_embd, head_size, dropout, focus_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"The quick brown fox jumped over the lazy dog\"\n",
    "# # sentence_tokens = tokenizer.encode(sentence).ids  # Convert to token IDs\n",
    "# # sentence_tensor = torch.tensor(sentence_tokens).unsqueeze(0).unsqueeze(-1)  # Add batch and feature dimensions\n",
    "\n",
    "# # Training step\n",
    "# loss = trainer.train_step(sentence, n_sims=5)\n",
    "# print(f\"Training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence tokens:  [462, 4794, 26347, 20930, 13983, 757, 906, 6247, 77, 4310]\n",
      "data:  tensor([    2,   462,  4794, 26347, 20930, 13983,   757,   906,  6247,    77,\n",
      "         4310,     3])\n",
      "data shape:  torch.Size([12])\n",
      "index size:  torch.Size([256])\n",
      "x shape:  torch.Size([256, 21])\n",
      "attended shape:  torch.Size([256, 21])\n",
      "attended:  tensor([[-0.5806, -2.4734, -0.2571,  ...,  2.7967, -0.5955, -0.1159],\n",
      "        [ 0.9965,  0.1544, -1.6804,  ...,  1.6156, -0.7192,  0.0615],\n",
      "        [ 0.2554, -0.1855, -0.3438,  ...,  1.9990, -0.9675,  0.7281],\n",
      "        ...,\n",
      "        [-0.0273, -0.8210,  1.1588,  ..., -0.3338, -1.7046, -0.3155],\n",
      "        [-1.0899, -1.4467,  1.2505,  ...,  1.8580, -0.2941,  0.8575],\n",
      "        [ 1.3017,  0.1894,  1.1948,  ..., -0.0077, -0.3689, -0.0892]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "[1, 1, 1, 1]\n",
      "policy_probs: [3.2711669e-05 4.0513132e-05 3.2647658e-05 ... 2.9849527e-05 3.7365779e-05\n",
      " 3.3938712e-05], shape: (570000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bennyrose/Little-Guy/lg/alpha.py:115: RuntimeWarning: invalid value encountered in divide\n",
      "  policy_target = visit_counts / visit_counts.sum()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[10]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumped over the lazy dog\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# sentence_tokens = tokenizer.encode(sentence).ids  # Convert to token IDs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# sentence_tensor = torch.tensor(sentence_tokens).unsqueeze(0).unsqueeze(-1)  # Add batch and feature dimensions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Little-Guy/lg/alpha.py:174\u001b[0m, in \u001b[0;36mAlphaZeroTrainer.train_step\u001b[0;34m(self, sentence, n_sims, focus_len)\u001b[0m\n\u001b[1;32m    171\u001b[0m sentence_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sentence_tokens, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Step 4: Forward pass through the model\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Step 5: Backward pass and optimization\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Little-Guy/lg/arch.py:164\u001b[0m, in \u001b[0;36mLGGPT.train_forward\u001b[0;34m(self, embeds, index, targets)\u001b[0m\n\u001b[1;32m    162\u001b[0m B1, T1, C1 \u001b[38;5;241m=\u001b[39m logits1\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    163\u001b[0m logits1 \u001b[38;5;241m=\u001b[39m logits1\u001b[38;5;241m.\u001b[39mview(B1\u001b[38;5;241m*\u001b[39mT1, C1)\n\u001b[0;32m--> 164\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB1\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mT1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits1, targets)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# B2, T2, C2 = logits2.shape\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# logits2 = logits2.view(B2*T2, C2)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# targets = targets.view(B2*T2)\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# loss2 = F.cross_entropy(logits2, targets)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[10]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumped over the lazy dog\"\n",
    "# sentence_tokens = tokenizer.encode(sentence).ids  # Convert to token IDs\n",
    "# sentence_tensor = torch.tensor(sentence_tokens).unsqueeze(0).unsqueeze(-1)  # Add batch and feature dimensions\n",
    "\n",
    "# Training step\n",
    "loss = trainer.train_step(sentence, n_sims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dropout, sentence, single_head, tokenizer, trainer, vocab\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "littleguy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
